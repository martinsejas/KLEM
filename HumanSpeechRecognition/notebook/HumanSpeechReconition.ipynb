{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_TRAIN = \"C:/Users/KB/OneDrive/Desktop/Identify-your-Own-Voice-main/dataset/train\"\n",
    "DATA_PATH_TEST = \"C:/Users/KB/OneDrive/Desktop/Identify-your-Own-Voice-main/dataset/test\"\n",
    "SAMPLE_RATE = 22050\n",
    "MFCC_COUNT = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfccs(file_path):\n",
    "    audio, _ = librosa.load(file_path, sr=SAMPLE_RATE, mono=True)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=MFCC_COUNT)\n",
    "    return np.mean(mfccs.T, axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    mfccs = []\n",
    "    labels = []\n",
    "    for label in [\"human\", \"other\"]:\n",
    "        folder_path = os.path.join(data_path, label)\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            mfcc = extract_mfccs(file_path)\n",
    "            mfccs.append(mfcc)\n",
    "            labels.append(label)\n",
    "    return np.array(mfccs), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(data_path):\n",
    "    mfccs = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(data_path):\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        mfcc = extract_mfccs(file_path)\n",
    "        mfccs.append(mfcc)\n",
    "        filenames.append(filename)\n",
    "    return np.array(mfccs), filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KB\\AppData\\Local\\Temp\\ipykernel_9680\\4068260846.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(file_path, sr=SAMPLE_RATE, mono=True)\n",
      "c:\\Users\\KB\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "c:\\Users\\KB\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\spectrum.py:256: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "X, y = load_data(DATA_PATH_TRAIN)\n",
    "\n",
    "# Convert labels from text to integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_shape=(MFCC_COUNT,)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Build a simple model\n",
    "model = Sequential([\n",
    "    Dense(100, activation='selu', kernel_initializer=\"lecun_normal\", input_shape=(MFCC_COUNT,)),\n",
    "    Dense(50, activation='selu', kernel_initializer=\"lecun_normal\"),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "216/216 [==============================] - 2s 4ms/step - loss: 0.6811 - accuracy: 0.8620 - val_loss: 0.5850 - val_accuracy: 0.8292\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.3131 - accuracy: 0.9072 - val_loss: 0.2823 - val_accuracy: 0.9114\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.2328 - accuracy: 0.9240 - val_loss: 0.3819 - val_accuracy: 0.8894\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.1955 - accuracy: 0.9329 - val_loss: 0.2014 - val_accuracy: 0.9259\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.2795 - accuracy: 0.9225 - val_loss: 0.2234 - val_accuracy: 0.9189\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.1870 - accuracy: 0.9376 - val_loss: 0.2347 - val_accuracy: 0.9160\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.2179 - accuracy: 0.9302 - val_loss: 0.1828 - val_accuracy: 0.9375\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.1894 - accuracy: 0.9363 - val_loss: 0.2003 - val_accuracy: 0.9311\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.1573 - accuracy: 0.9468 - val_loss: 0.2805 - val_accuracy: 0.9155\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.1823 - accuracy: 0.9384 - val_loss: 0.2242 - val_accuracy: 0.9241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe4dd3dae0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 2ms/step\n",
      "Confusion Matrix:\n",
      "[[ 292   33]\n",
      " [  98 1304]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.90      0.82       325\n",
      "           1       0.98      0.93      0.95      1402\n",
      "\n",
      "    accuracy                           0.92      1727\n",
      "   macro avg       0.86      0.91      0.88      1727\n",
      "weighted avg       0.93      0.92      0.93      1727\n",
      "\n",
      "Model Accuracy: 92.41%\n",
      "Model Loss: 0.22\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_val_pred_prob = model.predict(X_val)\n",
    "y_val_pred = (y_val_pred_prob > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate the classification report\n",
    "cr = classification_report(y_val, y_val_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(cr)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(\"Model Accuracy: {:.2f}%\".format(accuracy*100))\n",
    "print(\"Model Loss: {:.2f}\".format(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_audio_class(audio_file_path, model, le):\n",
    "    mfcc = extract_mfccs(audio_file_path)\n",
    "    mfcc = np.expand_dims(mfcc, axis=0)  # because the model expects 2D array\n",
    "    prediction_prob = model.predict(mfcc)\n",
    "    prediction = (prediction_prob > 0.5).astype(\"int32\")\n",
    "    prediction_label = le.inverse_transform(prediction)[0]\n",
    "    return prediction_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "The audio is predicted as: other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KB\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Make prediction on a single file\n",
    "audio_file_path = \"C:/Users/KB/OneDrive/Desktop/Identify-your-Own-Voice-main/dataset/test/testaudio (1987).wav\"  # Adjust if necessary\n",
    "prediction = predict_audio_class(audio_file_path, model, le)\n",
    "print(f\"The audio is predicted as: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_model/ASR\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./my_model/ASR\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save (\"./my_model/ASR\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
